[프로젝트 ]
1) 환경설정
2) 데이터 준비 
(1) es 인덱싱
(2) rdb reshitf 


[프로젝트 사전 준비]

1. 프로그램
포맷 안해도 되니까 .. 최대한 파일 넣어서 가져가기  

2. 엘라스틱 
정리 (긴가민가요한부분 다시 학습)

3. 세션에서 나왔던 중요 포인트들 다시 정리 하기




# analyzer 
es는 문자열 필드가 저장될 때 데이터에서 검색어 토큰을 저장하기 위해
여러 단계의 처리 과정을 거친다.
이 전체 과정을 텍스트분석 (Text Analysis) 라고 하고 이 과정을 처리하는
기능을 애널라이저 (Analyzer) fkrh gksek. 
es의 애널라이저는 0~3개의 캐릭터 필터와 1개의 토크나이저, 그리고 0~n개의 
토큰 필터로 이루어진다.

# 1. 캐릭터필터
텍스트 데이터가 입력되면 가장 먼저 필요에 따라 전체 문장에서 특정 문자를 대치하거나
제거하는데 이 과정을 담당하는 기능이 캐릭터 필터 
# 2. 토크나이저
문장에 속한 단어들을 텀 단위로 하나씩 분리해내는 처리과정이 토크나이저 
# 3. 토큰필터
분리된 텀들을 하나씩 가공하는 과정이 토큰 필터 

# term 쿼리
match와 문법은 유사하지만 term 쿼리는 입력한 검색어는 애널라이저를 
적용하지 않고 입력된 검색어 그대로 일치하는 텀을 찾는다. 

# mapping 
mapping 캐릭터 필터를 이용하면 지정한 단어를 다른 단어로 치환 가능 
특수문자 등을 포함하는 검색기능을 구현하려는 경우 반드시 ㅈ

c++ -> c_plus_plus 


색인 analysis 
검색 analysis 
어떤 것을 쓸것인 지 
setting, mapping 
filter, tokenizer
[nori ]

sudo bin/elasticsearch-plugin install analysis-nori
[노드 롤]
node.roles: [master, data, data_content, data_hot, data_warm, data_cold, data_frozen, ingest, ml, remote_cluster_client, transform]

총 8개의 vpc
마스터 노드 1개
후보마스터
데이터 

- master 관리자역할
클러스터 내 노드 추가 및 삭제, 인덱스생성수정삭제 , 샤드할당, 클러스터상태관리...

master  eligible 마스터 후보 노드 
최소 3대이상 권장 
(voting only인 경우를 제외하면 어떤 마스터 후보 노드던 마스터 노드로 선출될
자격을 가지고 마스터 노드의 역할을 수행 )
마스터 노드가 관리하는 클러스터 메타 데이터를 공유 받아 마스터 노드 장애
발생 시 즉각 대체 가능 

- data 
물리적으로 데이터 저장하는 노드 
즉, 인덱싱된 문서를 샤드 단위의 세그먼트로 저장하는 노드이다.
crud작업, 검색 및 집계와 같은 데이터 관련 작업 처리 

voting_only
마스터 역할을 수행할 수 없는 마스터 후보 노드 
- data_content
대표적인 유스케이스는 시계절 데이터를 다룰 때 클러스터 토폴리지를 용도에
맞게 여러개의 티어로 분리 
- data_hot  새로 저장되는 최신의 데이터
- data_warm 한달정도 지난 데이터
- data_cold 몇달이 지나서 분석에 빈번히 사용되지 않는 데이터
- data_frozen 유료기능 
- ingest
문서의 가공과 정제를 위한 인제스트 파이프라인이 실행되는 노드 
모든 노드는 기본적으로 인제스트 노드 역할을 수행할 수 있지만, 
데이터 처리를 많이 해야하는 클러스터의 경유에는 별도의 인제스트 전용 노드 구성을 추천

작은 규모의 클러스터인 경우 최적화를 보통 위해 마스터 노드와 데이터 노드만 고려 
인제스트 파이프라인을 따로 사용하지 않기 때문에 노드 역할에 인제스트 역할을 부여하지 않느 경우가 많다 
별도로 인제스트 파이프라인을 사용하지 않는다고 해도 시스템 인덱스가 사용하는 경우가 있으므로
같이 설정하는 것이 좋다.

logstash는 단일 노드에서만 실행되므로 분산처리가 불가능하다.
elasticsearch의 인제스트 노드는 클러스터링으로 분선가능


Coordinating node
기본적으로 모든 노드가 rest api요청을 처리한다. 하지만 대규모의 클러스터경우
마스터노드 또는 데이터 노드가 부하가 많다면 rest api요청만 딸 ㅗ처리하는
코디네이팅 노드를 지정하여 처리 가능 
코디네이팅 노드가 사용자 요청을 받아서 각 노드들에 전달하고 취합해
결과를 제공한다. 
코디네이팅 전용 노드를 두게 되면 로드밸런싱, 요청 라우팅, 요청 캐싱, 각 노드에서
계산된 결과에 대한 취합헤 집중한다. 따라서 코디네이팅 전용 노드를 따로 세팅할 경우
데이터 노드의 부하를 줄이고 검색효율을 높일 수 있다.

마스터,데이터,인제스트가 false로 되어있어야함

[유료]
- remote_cluster_client
 원격으로 다른 클러스터를 컨트롤하기 위하 노드 
- ml
 ml기능을 수행하기 위한 노드 
- transform
통계작업과 같은 transform 기능을 위한 노드 




[샤드의 개수 ]


1. 샤드 최적화 
클러스터는 인덱스를 최대 몇 개 까지 생성할 수 있을까? 하나의 샤드 크기는 과연 얼마가 적당할까 ? 

운영중인 인덱스의 샤드 개수는 원칙적으로 수정이 불가능하다 하지만 서비스를 운영하다 보면 데이터 크기는 점점 더 커지고 
많은 데이터가 쌓이면 샤드의 부하 문제가 발생할 수 있다. 인덱스를 생성할 때 한번 설정된 샤드의 개수는 절대 변경이 불가능하기 때문에
데이터의 크기가 최대 얼마까지 증가할 것인지를 사전에 잘 계산해서 최초 인덱스를 생성할 때 샤드의 개수를 신중하게 결정해야한다.

* 샤드 개수를 변경 못하는 이유?
루씬은 단일 머신 위에서만 동작하는 검색엔진으로 함께 인덱스를 구성하는 다른 샤드의 존재를 알지못한다. 
어떻게든 바꾼다 하면 reindex api를 활용하여 인덱스를 새롭게 생성하고 전체 데이터를 처음부터 다시 재색인 하는 방법이 존재한다.


엘라스틱서치에는 두가지 종류의 샤드가 공존한다.
첫번째는 실제 서비스가 일어나는 프라이머리 샤드 , 클러스터에에서 실질적인 CRUD를 제공하는 샤드로서 엘라스틱서치를 구성하는 핵심 요소
프라이머리샤드를 일반적으로 샤드라고 줄여서 부른다. 두번째는 레플리카 샤드다 레플리카 샤드는 장애복구를 위해 존재 
하지만 프라이머리 샤드와 동일한 데이터를 가지고 있기 때문에 평상시에는 읽기 분산에 활용된다. 
es에서는 최초 인덱스를 생성할 때 settings 속성을 이용해 샤드와 레플리카 개수를 각각 정의할 수 있다 
샤드의 개수는 전체 데이터를 몇 개의 샤드로 나눠서 보관할 것인지를 의미하고 , 레플리카의 개수는 몇개의 복사본 세트를 만들것인지를 의미한다. 


PUT /movie
{
    "settings": {
        "index" : {
            "number_of_shards": 5,
            "number_of_replicas": 1
        }
    }
}

데이터를 5개의 샤드로 분산하고 1개의 레플리카 세트를 생성하도록 설정 이렇게 설정된 인덱스가 생성되면 5개의 프라이머리 샤드와 5개의 레플리카 샤드가 생성되어
인덱스 내부에는 총 10개의 샤드가 존재 


레플리카의 샤드의 복제본 수는 얼마가 적당할까 ? 
레플리카는 얼마든지 변경이 가능하기 때문에 기본적으로 1개이상의 복제본 세트를 사용한다. 
레플리카가 많아질수록 색인 성능은 떨어지기 때문에
읽기 분산이 중요한 경우에는 색인 성능을 일부 포기하고 레플리카 세트의 수를 늘리는것이 좋고 , 빠른 색인이 중요한 경우에는 읽기 분산을 일부 포기하고
레플리카 세트의 수를 최소화 

q. 클러스터에서 운영 가능한 최대 샤드 수는?
전체 샤드 수에 대한 특별한 제한은 없다 
클러스터에는 인덱스는 무한대로 생성 가능 , 다만 개별 인덱스를 생성할때 설정 가능한 샤드의 수는 1024개

클러스터에 많은 수의 샤드가 존재할 경우
모든 샤드는 마스터 노드에서 관리가 되므로 샤드가 많아질수록 마스터 노드의 부하도 덩달아서 증가. 검색이나 색인같은 작업도 느려질수 있다
심각한 경우에는 마스터 노드가 마비될 수 있다. 

샤드의 물리적인 크기와 복구 시간
노드에 장애가 발생하면 장애가 발생한 프라이머리 샤드와 동일한 데이터를 가지고 있는 레플리카 샤드가 순간적으로 프라이머리 샤드로 전환되어 서비스 
그와 동시에 프라이머리 샤드로 전환된 샤드와 동일한 샤드가 물리적으로 다른 장비에서 레플리카 샤드로 새롭게 생성된다. 그리고 서비스는 한동안 이대로 유지 
시간이 지나 장애가 발생한 노드가 복구되면 복구된 노드로 일부 샤드들이 네트워크를 통해 이동한다. 이러한 이동 과정을 통해 내부적으로 전체적인 클러스터의 균형을 맞춘다.
복구 시 샤드 단위로 데이터가 이동하기 때문에 샤드의 크기가 클 수록 복구 작업에 부정적인 영향

적절한 샤드의 크기 
적절한 샤드 크기는 정해진 공식은 없지만 샤드 1개가 물리적으로 50GB가 넘지않도록 권장한다.


2. 힙크기
엘라스틱서치는 메모리를 많이 사용하는 애플리케이션 시스템에서 제공되는 물리 메모리를 jvm 힙에 할당해서 엘라스틱서치가 사용하도록 설정할 수 있다
힙 메모리가 많을 수록 그에 비례해서 성능도 올라간다
다만, 너무 작은 힙 크기는 out-of-memory exception 반대로 너무 큰 힙 크기는 full gc가 발생할 때 시스템 전제가 마비되는 stw(stop the world)를 발생시킬수 있다.

기본은 1gb 설정인데 이는 테스트 값이라 운영환경에서는 jvm.options에서 xms(최소힙크기), xmx(최대힙크기) 설정값을 변경한다. 
힙이 부족하다 싶으면 최대 힙크기까지 늘어나는데, 이과정에서 애플리케이션의 성능 저하가 일어날 수 있기 때문에 최소힙크기와 최대힙크기를 같게 설정하는것이 유리하다.

적절한 힙크기는 es에서는 최댓값으로 32gb이하를 설정하는 것을 권장한다.

운영체제에 50% 메모리 공간을 보장하자 
샤드는 내부에 루씬을 가지고 있으며 루씬은 세그먼트 생성 및 관리를 위해 커널 시스템 캐시를 최대한 많이 활용하고 있다.
실시간 검색을 지원하기 위해서는 루씬이 최대한 많은 시스템 캐시를 확보하도록 지원
시스템 캐시는 운영체제가 가지고 있는 메모리 공간으로 커널 내부에 존재 
그러므로 물리적은 메모리 공간의 50%정도는 운영체제가 자유롭게 사용하도록 할당하고 나머지 50%정도를 엘라스틱 서치 힙으로 할당 

엘라스틱서치에서는 최신의 64비트 jvm위에서 compressed oop 방식으로 동작하는 32gb의 힙크기를 가지는 시스템을 가장 이상적인 구성으로 안내
일반적인 oop 방식으로 동작할 경우 동일한 효율을 내기 위해서는 대략적으로 40gb~50gb의 추가 메모리가 더 필요할 것이기 때문이다 
만약 수백 gb의 물리적인 메모리를 탑재한 서버가 있더라도 힙 크기를 32gb이상으로 설정하지 말아야한다. 메모리의 낭비도 심해지고 gc에도 많은 부담을 주게 되므로차라리 하나의 물리적인 서버에서 
다수의 엘라스틱서치 인스턴스를 새엇ㅇ하는편이 좋다. 

* compressed ordinary object pointer
jdk7부터는 기본 기능 
포인터의 공간 낭비를 줄이고 좀 더 빠른 연산을 위해 포인터를 압축하여 표현하는 일종의 트릭
핵심 원리는 포인터가 객체의 정확한 메모리 주스를 기리키게 하는 것이 아니라 상대적인 오브젝트 오프셋을 가리키도록 변형하여 동작 
더 많은 메모리 공간을 카리킬 수 있게 된다. 






